{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYU CDS\n",
    "\n",
    "### Fall 2021\n",
    "\n",
    "### Introduction to Data Science\n",
    "\n",
    "### Project 2\n",
    "\n",
    "### student netid: sy3420\n",
    "\n",
    "### deadline: Dec 06, 2021, 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data analysis Project 2\n",
    "### Correlation and Regression of Movie Ratings Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "This dataset features ratings data of 400 movies from 1097 research participants. \n",
    "\n",
    "* 1st row: Headers (Movie titles/questions) â€“ note that the indexing in this list is from 1\n",
    "* Row 2-1098: Responses from individual participants\n",
    "* Columns 1-400: These columns contain the ratings for the 400 movies (0 to 4, and missing)\n",
    "* Columns 401-421: These columns contain self-assessments on sensation seeking behaviors (1-5)\n",
    "* Columns 422-464: These columns contain responses to personality questions (1-5)\n",
    "* Columns 465-474: These columns contain self-reported movie experience ratings (1-5)\n",
    "* Column 475: Gender identity (1 = female, 2 = male, 3 = self-described)\n",
    "* Column 476: Only child (1 = yes, 0 = no, -1 = no response)\n",
    "* Column 477: Movies are best enjoyed alone (1 = yes, 0 = no, -1 = no response)\n",
    "\n",
    "Note that we did most of the data munging for you already (e.g. Python interprets commas in a csv file as separators, so we removed all commas from movie titles), but you still need to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import copy\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Q1:\n",
    "\n",
    "\n",
    "**Note:** For all missing values in the data, use the average of the corresponding column so to fill in the missing data. \n",
    "\n",
    "\n",
    "\n",
    "In this problem, under **the most correlated**, we consider the largest correlation in the absolute value.\n",
    "\n",
    "\n",
    "1.1. For every user in the given data, find its most correlated user. \n",
    "\n",
    "1.2. What is the pair of the most correlated users in the data? \n",
    "\n",
    "1.3. What is the value of this highest correlation?\n",
    "\n",
    "1.4. For users 0, 1, 2, \\dots, 9, print their most correlated users. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv(\"movieReplicationSet.csv\")\n",
    "movies = movie.iloc[:,:400]\n",
    "movies_clean = movies\n",
    "\n",
    "#filling in missing data\n",
    "for column in movies:\n",
    "    movies_clean[column] = movies[column].fillna(np.mean(movies[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Life of David Gale (2003)</th>\n",
       "      <th>Wing Commander (1999)</th>\n",
       "      <th>Django Unchained (2012)</th>\n",
       "      <th>Alien (1979)</th>\n",
       "      <th>Indiana Jones and the Last Crusade (1989)</th>\n",
       "      <th>Snatch (2000)</th>\n",
       "      <th>Rambo: First Blood Part II</th>\n",
       "      <th>Fargo (1996)</th>\n",
       "      <th>Let the Right One In (2008)</th>\n",
       "      <th>Black Swan (2010)</th>\n",
       "      <th>...</th>\n",
       "      <th>X-Men 2 (2003)</th>\n",
       "      <th>The Usual Suspects (1995)</th>\n",
       "      <th>The Mask (1994)</th>\n",
       "      <th>Jaws (1975)</th>\n",
       "      <th>Harry Potter and the Chamber of Secrets (2002)</th>\n",
       "      <th>Patton (1970)</th>\n",
       "      <th>Anaconda (1997)</th>\n",
       "      <th>Twister (1996)</th>\n",
       "      <th>MacArthur (1977)</th>\n",
       "      <th>Look Who's Talking (1989)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>2.778618</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>3.153422</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>2.778618</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>3.272459</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>3.153422</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.153422</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>3.153422</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>2.778618</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>3.153422</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>2.778618</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>2.899606</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>2.911565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>2.618952</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2.151316</td>\n",
       "      <td>2.021127</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.707612</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.49635</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.914062</td>\n",
       "      <td>3.101036</td>\n",
       "      <td>2.559045</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.278689</td>\n",
       "      <td>2.295455</td>\n",
       "      <td>2.402299</td>\n",
       "      <td>2.114754</td>\n",
       "      <td>2.337963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      The Life of David Gale (2003)  Wing Commander (1999)  \\\n",
       "0                          2.151316               2.021127   \n",
       "1                          2.151316               2.021127   \n",
       "2                          2.151316               2.021127   \n",
       "3                          2.151316               2.021127   \n",
       "4                          2.151316               2.021127   \n",
       "...                             ...                    ...   \n",
       "1092                       2.151316               2.021127   \n",
       "1093                       3.000000               4.000000   \n",
       "1094                       2.151316               2.021127   \n",
       "1095                       2.151316               2.021127   \n",
       "1096                       2.151316               2.021127   \n",
       "\n",
       "      Django Unchained (2012)  Alien (1979)  \\\n",
       "0                    4.000000      2.707612   \n",
       "1                    1.500000      2.707612   \n",
       "2                    3.153422      2.707612   \n",
       "3                    2.000000      2.707612   \n",
       "4                    3.500000      2.707612   \n",
       "...                       ...           ...   \n",
       "1092                 3.153422      2.707612   \n",
       "1093                 3.153422      2.707612   \n",
       "1094                 3.153422      2.707612   \n",
       "1095                 3.153422      2.707612   \n",
       "1096                 4.000000      2.707612   \n",
       "\n",
       "      Indiana Jones and the Last Crusade (1989)  Snatch (2000)  \\\n",
       "0                                      3.000000       2.597656   \n",
       "1                                      2.778618       2.597656   \n",
       "2                                      2.778618       2.597656   \n",
       "3                                      3.000000       2.597656   \n",
       "4                                      0.500000       2.597656   \n",
       "...                                         ...            ...   \n",
       "1092                                   3.500000       2.597656   \n",
       "1093                                   4.000000       4.000000   \n",
       "1094                                   2.778618       2.597656   \n",
       "1095                                   2.778618       2.597656   \n",
       "1096                                   2.500000       2.597656   \n",
       "\n",
       "      Rambo: First Blood Part II  Fargo (1996)  Let the Right One In (2008)  \\\n",
       "0                       2.365385      2.899606                      2.49635   \n",
       "1                       2.365385      2.899606                      2.49635   \n",
       "2                       2.365385      2.899606                      2.49635   \n",
       "3                       2.365385      2.899606                      2.49635   \n",
       "4                       0.500000      1.000000                      2.49635   \n",
       "...                          ...           ...                          ...   \n",
       "1092                    2.365385      2.899606                      2.49635   \n",
       "1093                    2.500000      2.899606                      3.50000   \n",
       "1094                    2.365385      3.500000                      2.49635   \n",
       "1095                    2.365385      2.899606                      2.49635   \n",
       "1096                    2.365385      3.000000                      2.49635   \n",
       "\n",
       "      Black Swan (2010)  ...  X-Men 2 (2003)  The Usual Suspects (1995)  \\\n",
       "0              2.911565  ...        2.914062                   3.101036   \n",
       "1              2.911565  ...        2.914062                   3.101036   \n",
       "2              2.911565  ...        2.914062                   3.101036   \n",
       "3              4.000000  ...        2.914062                   3.000000   \n",
       "4              0.000000  ...        2.500000                   3.101036   \n",
       "...                 ...  ...             ...                        ...   \n",
       "1092           2.911565  ...        2.914062                   3.101036   \n",
       "1093           3.500000  ...        4.000000                   3.101036   \n",
       "1094           2.911565  ...        2.914062                   3.101036   \n",
       "1095           2.911565  ...        2.914062                   3.101036   \n",
       "1096           3.500000  ...        2.914062                   3.101036   \n",
       "\n",
       "      The Mask (1994)  Jaws (1975)  \\\n",
       "0            2.559045     4.000000   \n",
       "1            2.559045     2.618952   \n",
       "2            2.559045     2.618952   \n",
       "3            2.559045     2.618952   \n",
       "4            3.000000     2.618952   \n",
       "...               ...          ...   \n",
       "1092         2.559045     2.618952   \n",
       "1093         4.000000     3.500000   \n",
       "1094         2.559045     2.618952   \n",
       "1095         2.559045     2.618952   \n",
       "1096         2.559045     3.500000   \n",
       "\n",
       "      Harry Potter and the Chamber of Secrets (2002)  Patton (1970)  \\\n",
       "0                                           0.500000       2.278689   \n",
       "1                                           4.000000       2.278689   \n",
       "2                                           3.500000       2.278689   \n",
       "3                                           2.500000       2.278689   \n",
       "4                                           3.272459       2.278689   \n",
       "...                                              ...            ...   \n",
       "1092                                        4.000000       2.278689   \n",
       "1093                                        3.500000       4.000000   \n",
       "1094                                        4.000000       2.278689   \n",
       "1095                                        2.500000       2.278689   \n",
       "1096                                        4.000000       2.278689   \n",
       "\n",
       "      Anaconda (1997)  Twister (1996)  MacArthur (1977)  \\\n",
       "0            2.295455        2.402299          2.114754   \n",
       "1            2.295455        2.402299          2.114754   \n",
       "2            2.295455        2.402299          2.114754   \n",
       "3            2.295455        2.402299          2.114754   \n",
       "4            2.295455        1.500000          2.114754   \n",
       "...               ...             ...               ...   \n",
       "1092         2.295455        2.402299          2.114754   \n",
       "1093         3.500000        4.000000          4.000000   \n",
       "1094         2.295455        2.402299          2.114754   \n",
       "1095         2.295455        2.402299          2.114754   \n",
       "1096         2.295455        2.402299          2.114754   \n",
       "\n",
       "      Look Who's Talking (1989)  \n",
       "0                      2.337963  \n",
       "1                      2.337963  \n",
       "2                      2.337963  \n",
       "3                      2.337963  \n",
       "4                      2.337963  \n",
       "...                         ...  \n",
       "1092                   2.337963  \n",
       "1093                   4.000000  \n",
       "1094                   2.337963  \n",
       "1095                   2.337963  \n",
       "1096                   2.337963  \n",
       "\n",
       "[1097 rows x 400 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1087</th>\n",
       "      <th>1088</th>\n",
       "      <th>1089</th>\n",
       "      <th>1090</th>\n",
       "      <th>1091</th>\n",
       "      <th>1092</th>\n",
       "      <th>1093</th>\n",
       "      <th>1094</th>\n",
       "      <th>1095</th>\n",
       "      <th>1096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.337763</td>\n",
       "      <td>0.323811</td>\n",
       "      <td>0.217614</td>\n",
       "      <td>0.368250</td>\n",
       "      <td>0.422829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.244606</td>\n",
       "      <td>0.350904</td>\n",
       "      <td>0.356829</td>\n",
       "      <td>0.458425</td>\n",
       "      <td>0.402888</td>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.458706</td>\n",
       "      <td>0.311671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.563834</td>\n",
       "      <td>0.531068</td>\n",
       "      <td>0.311732</td>\n",
       "      <td>0.645176</td>\n",
       "      <td>0.664096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399474</td>\n",
       "      <td>0.354794</td>\n",
       "      <td>0.513256</td>\n",
       "      <td>0.582502</td>\n",
       "      <td>0.660635</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.574365</td>\n",
       "      <td>0.720326</td>\n",
       "      <td>0.521499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.661554</td>\n",
       "      <td>0.512220</td>\n",
       "      <td>0.369103</td>\n",
       "      <td>0.646632</td>\n",
       "      <td>0.729210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489893</td>\n",
       "      <td>0.348640</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>0.664633</td>\n",
       "      <td>0.834644</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.773501</td>\n",
       "      <td>0.499990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>0.255438</td>\n",
       "      <td>0.387712</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>0.484603</td>\n",
       "      <td>0.292886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095055</td>\n",
       "      <td>0.365389</td>\n",
       "      <td>0.345461</td>\n",
       "      <td>0.374784</td>\n",
       "      <td>0.340569</td>\n",
       "      <td>0.345736</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.261623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.216660</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.340718</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>0.290386</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>0.327620</td>\n",
       "      <td>0.348971</td>\n",
       "      <td>0.385944</td>\n",
       "      <td>0.108995</td>\n",
       "      <td>0.392484</td>\n",
       "      <td>0.365428</td>\n",
       "      <td>0.264850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.402888</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.345736</td>\n",
       "      <td>0.385944</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.390381</td>\n",
       "      <td>0.312582</td>\n",
       "      <td>0.628361</td>\n",
       "      <td>0.648463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390270</td>\n",
       "      <td>0.311088</td>\n",
       "      <td>0.463884</td>\n",
       "      <td>0.646834</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.696046</td>\n",
       "      <td>0.730998</td>\n",
       "      <td>0.439620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.108995</td>\n",
       "      <td>0.130190</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.166927</td>\n",
       "      <td>0.137077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055893</td>\n",
       "      <td>0.190506</td>\n",
       "      <td>0.187977</td>\n",
       "      <td>0.149024</td>\n",
       "      <td>0.186592</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.077180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.574365</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.392484</td>\n",
       "      <td>0.548735</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.292073</td>\n",
       "      <td>0.529138</td>\n",
       "      <td>0.574514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355428</td>\n",
       "      <td>0.294454</td>\n",
       "      <td>0.491842</td>\n",
       "      <td>0.549795</td>\n",
       "      <td>0.605292</td>\n",
       "      <td>0.696046</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.658970</td>\n",
       "      <td>0.400818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.458706</td>\n",
       "      <td>0.720326</td>\n",
       "      <td>0.773501</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.365428</td>\n",
       "      <td>0.639427</td>\n",
       "      <td>0.489932</td>\n",
       "      <td>0.338069</td>\n",
       "      <td>0.598056</td>\n",
       "      <td>0.644131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408570</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.546651</td>\n",
       "      <td>0.669349</td>\n",
       "      <td>0.731422</td>\n",
       "      <td>0.730998</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.658970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>0.311671</td>\n",
       "      <td>0.521499</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.264850</td>\n",
       "      <td>0.256234</td>\n",
       "      <td>0.376801</td>\n",
       "      <td>0.177096</td>\n",
       "      <td>0.516993</td>\n",
       "      <td>0.345430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249155</td>\n",
       "      <td>0.282750</td>\n",
       "      <td>0.259530</td>\n",
       "      <td>0.302109</td>\n",
       "      <td>0.366726</td>\n",
       "      <td>0.439620</td>\n",
       "      <td>0.077180</td>\n",
       "      <td>0.400818</td>\n",
       "      <td>0.468334</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 1097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.453172  0.460725  0.224024  0.258664  0.337763  0.323811   \n",
       "1     0.453172  1.000000  0.785843  0.404253  0.374899  0.563834  0.531068   \n",
       "2     0.460725  0.785843  1.000000  0.358657  0.443320  0.661554  0.512220   \n",
       "3     0.224024  0.404253  0.358657  1.000000  0.198358  0.255438  0.387712   \n",
       "4     0.258664  0.374899  0.443320  0.198358  1.000000  0.311742  0.216660   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1092  0.402888  0.687942  0.818988  0.345736  0.385944  0.592500  0.390381   \n",
       "1093  0.045282  0.188574  0.198594  0.101303  0.108995  0.130190  0.147645   \n",
       "1094  0.313826  0.574365  0.715653  0.366085  0.392484  0.548735  0.350640   \n",
       "1095  0.458706  0.720326  0.773501  0.279389  0.365428  0.639427  0.489932   \n",
       "1096  0.311671  0.521499  0.499990  0.261623  0.264850  0.256234  0.376801   \n",
       "\n",
       "          7         8         9     ...      1087      1088      1089  \\\n",
       "0     0.217614  0.368250  0.422829  ...  0.249962  0.244606  0.350904   \n",
       "1     0.311732  0.645176  0.664096  ...  0.399474  0.354794  0.513256   \n",
       "2     0.369103  0.646632  0.729210  ...  0.489893  0.348640  0.520503   \n",
       "3     0.267130  0.484603  0.292886  ...  0.095055  0.365389  0.345461   \n",
       "4     0.185430  0.340718  0.322100  ...  0.202366  0.290386  0.259655   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1092  0.312582  0.628361  0.648463  ...  0.390270  0.311088  0.463884   \n",
       "1093  0.106326  0.166927  0.137077  ...  0.055893  0.190506  0.187977   \n",
       "1094  0.292073  0.529138  0.574514  ...  0.355428  0.294454  0.491842   \n",
       "1095  0.338069  0.598056  0.644131  ...  0.408570  0.401600  0.546651   \n",
       "1096  0.177096  0.516993  0.345430  ...  0.249155  0.282750  0.259530   \n",
       "\n",
       "          1090      1091      1092      1093      1094      1095      1096  \n",
       "0     0.356829  0.458425  0.402888  0.045282  0.313826  0.458706  0.311671  \n",
       "1     0.582502  0.660635  0.687942  0.188574  0.574365  0.720326  0.521499  \n",
       "2     0.664633  0.834644  0.818988  0.198594  0.715653  0.773501  0.499990  \n",
       "3     0.374784  0.340569  0.345736  0.101303  0.366085  0.279389  0.261623  \n",
       "4     0.327620  0.348971  0.385944  0.108995  0.392484  0.365428  0.264850  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1092  0.646834  0.760200  1.000000  0.146274  0.696046  0.730998  0.439620  \n",
       "1093  0.149024  0.186592  0.146274  1.000000  0.146055  0.207358  0.077180  \n",
       "1094  0.549795  0.605292  0.696046  0.146055  1.000000  0.658970  0.400818  \n",
       "1095  0.669349  0.731422  0.730998  0.207358  0.658970  1.000000  0.468334  \n",
       "1096  0.302109  0.366726  0.439620  0.077180  0.400818  0.468334  1.000000  \n",
       "\n",
       "[1097 rows x 1097 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor = np.transpose(movies_clean).corr().abs()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1087</th>\n",
       "      <th>1088</th>\n",
       "      <th>1089</th>\n",
       "      <th>1090</th>\n",
       "      <th>1091</th>\n",
       "      <th>1092</th>\n",
       "      <th>1093</th>\n",
       "      <th>1094</th>\n",
       "      <th>1095</th>\n",
       "      <th>1096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.337763</td>\n",
       "      <td>0.323811</td>\n",
       "      <td>0.217614</td>\n",
       "      <td>0.368250</td>\n",
       "      <td>0.422829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.244606</td>\n",
       "      <td>0.350904</td>\n",
       "      <td>0.356829</td>\n",
       "      <td>0.458425</td>\n",
       "      <td>0.402888</td>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.458706</td>\n",
       "      <td>0.311671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.563834</td>\n",
       "      <td>0.531068</td>\n",
       "      <td>0.311732</td>\n",
       "      <td>0.645176</td>\n",
       "      <td>0.664096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399474</td>\n",
       "      <td>0.354794</td>\n",
       "      <td>0.513256</td>\n",
       "      <td>0.582502</td>\n",
       "      <td>0.660635</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.574365</td>\n",
       "      <td>0.720326</td>\n",
       "      <td>0.521499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.661554</td>\n",
       "      <td>0.512220</td>\n",
       "      <td>0.369103</td>\n",
       "      <td>0.646632</td>\n",
       "      <td>0.729210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489893</td>\n",
       "      <td>0.348640</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>0.664633</td>\n",
       "      <td>0.834644</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.773501</td>\n",
       "      <td>0.499990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>0.255438</td>\n",
       "      <td>0.387712</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>0.484603</td>\n",
       "      <td>0.292886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095055</td>\n",
       "      <td>0.365389</td>\n",
       "      <td>0.345461</td>\n",
       "      <td>0.374784</td>\n",
       "      <td>0.340569</td>\n",
       "      <td>0.345736</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.261623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.216660</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.340718</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202366</td>\n",
       "      <td>0.290386</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>0.327620</td>\n",
       "      <td>0.348971</td>\n",
       "      <td>0.385944</td>\n",
       "      <td>0.108995</td>\n",
       "      <td>0.392484</td>\n",
       "      <td>0.365428</td>\n",
       "      <td>0.264850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.402888</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.345736</td>\n",
       "      <td>0.385944</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.390381</td>\n",
       "      <td>0.312582</td>\n",
       "      <td>0.628361</td>\n",
       "      <td>0.648463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390270</td>\n",
       "      <td>0.311088</td>\n",
       "      <td>0.463884</td>\n",
       "      <td>0.646834</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.696046</td>\n",
       "      <td>0.730998</td>\n",
       "      <td>0.439620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.108995</td>\n",
       "      <td>0.130190</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.166927</td>\n",
       "      <td>0.137077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055893</td>\n",
       "      <td>0.190506</td>\n",
       "      <td>0.187977</td>\n",
       "      <td>0.149024</td>\n",
       "      <td>0.186592</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.077180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.574365</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.392484</td>\n",
       "      <td>0.548735</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.292073</td>\n",
       "      <td>0.529138</td>\n",
       "      <td>0.574514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355428</td>\n",
       "      <td>0.294454</td>\n",
       "      <td>0.491842</td>\n",
       "      <td>0.549795</td>\n",
       "      <td>0.605292</td>\n",
       "      <td>0.696046</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658970</td>\n",
       "      <td>0.400818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.458706</td>\n",
       "      <td>0.720326</td>\n",
       "      <td>0.773501</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.365428</td>\n",
       "      <td>0.639427</td>\n",
       "      <td>0.489932</td>\n",
       "      <td>0.338069</td>\n",
       "      <td>0.598056</td>\n",
       "      <td>0.644131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408570</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.546651</td>\n",
       "      <td>0.669349</td>\n",
       "      <td>0.731422</td>\n",
       "      <td>0.730998</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.658970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>0.311671</td>\n",
       "      <td>0.521499</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.264850</td>\n",
       "      <td>0.256234</td>\n",
       "      <td>0.376801</td>\n",
       "      <td>0.177096</td>\n",
       "      <td>0.516993</td>\n",
       "      <td>0.345430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249155</td>\n",
       "      <td>0.282750</td>\n",
       "      <td>0.259530</td>\n",
       "      <td>0.302109</td>\n",
       "      <td>0.366726</td>\n",
       "      <td>0.439620</td>\n",
       "      <td>0.077180</td>\n",
       "      <td>0.400818</td>\n",
       "      <td>0.468334</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 1097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.000000  0.453172  0.460725  0.224024  0.258664  0.337763  0.323811   \n",
       "1     0.453172  0.000000  0.785843  0.404253  0.374899  0.563834  0.531068   \n",
       "2     0.460725  0.785843  0.000000  0.358657  0.443320  0.661554  0.512220   \n",
       "3     0.224024  0.404253  0.358657  0.000000  0.198358  0.255438  0.387712   \n",
       "4     0.258664  0.374899  0.443320  0.198358  0.000000  0.311742  0.216660   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1092  0.402888  0.687942  0.818988  0.345736  0.385944  0.592500  0.390381   \n",
       "1093  0.045282  0.188574  0.198594  0.101303  0.108995  0.130190  0.147645   \n",
       "1094  0.313826  0.574365  0.715653  0.366085  0.392484  0.548735  0.350640   \n",
       "1095  0.458706  0.720326  0.773501  0.279389  0.365428  0.639427  0.489932   \n",
       "1096  0.311671  0.521499  0.499990  0.261623  0.264850  0.256234  0.376801   \n",
       "\n",
       "          7         8         9     ...      1087      1088      1089  \\\n",
       "0     0.217614  0.368250  0.422829  ...  0.249962  0.244606  0.350904   \n",
       "1     0.311732  0.645176  0.664096  ...  0.399474  0.354794  0.513256   \n",
       "2     0.369103  0.646632  0.729210  ...  0.489893  0.348640  0.520503   \n",
       "3     0.267130  0.484603  0.292886  ...  0.095055  0.365389  0.345461   \n",
       "4     0.185430  0.340718  0.322100  ...  0.202366  0.290386  0.259655   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1092  0.312582  0.628361  0.648463  ...  0.390270  0.311088  0.463884   \n",
       "1093  0.106326  0.166927  0.137077  ...  0.055893  0.190506  0.187977   \n",
       "1094  0.292073  0.529138  0.574514  ...  0.355428  0.294454  0.491842   \n",
       "1095  0.338069  0.598056  0.644131  ...  0.408570  0.401600  0.546651   \n",
       "1096  0.177096  0.516993  0.345430  ...  0.249155  0.282750  0.259530   \n",
       "\n",
       "          1090      1091      1092      1093      1094      1095      1096  \n",
       "0     0.356829  0.458425  0.402888  0.045282  0.313826  0.458706  0.311671  \n",
       "1     0.582502  0.660635  0.687942  0.188574  0.574365  0.720326  0.521499  \n",
       "2     0.664633  0.834644  0.818988  0.198594  0.715653  0.773501  0.499990  \n",
       "3     0.374784  0.340569  0.345736  0.101303  0.366085  0.279389  0.261623  \n",
       "4     0.327620  0.348971  0.385944  0.108995  0.392484  0.365428  0.264850  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1092  0.646834  0.760200  0.000000  0.146274  0.696046  0.730998  0.439620  \n",
       "1093  0.149024  0.186592  0.146274  0.000000  0.146055  0.207358  0.077180  \n",
       "1094  0.549795  0.605292  0.696046  0.146055  0.000000  0.658970  0.400818  \n",
       "1095  0.669349  0.731422  0.730998  0.207358  0.658970  0.000000  0.468334  \n",
       "1096  0.302109  0.366726  0.439620  0.077180  0.400818  0.468334  0.000000  \n",
       "\n",
       "[1097 rows x 1097 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I filled the diagonal values to 0 because I want to find the maximum correlation values for the questions below. \n",
    "#and since the diagonal values are all 1's, if i had to find a maximum value, I'd get all 1's.\n",
    "cor = np.transpose(movies_clean).corr().abs()\n",
    "np.fill_diagonal(cor.values, 0)\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 314, 1: 896, 2: 831, 3: 704, 4: 660, 5: 574, 6: 521, 7: 850, 8: 412, 9: 966, 10: 831, 11: 831, 12: 896, 13: 896, 14: 265, 15: 896, 16: 515, 17: 831, 18: 896, 19: 37, 20: 832, 21: 497, 22: 409, 23: 896, 24: 86, 25: 409, 26: 825, 27: 831, 28: 896, 29: 896, 30: 831, 31: 272, 32: 896, 33: 240, 34: 896, 35: 594, 36: 66, 37: 831, 38: 896, 39: 831, 40: 896, 41: 896, 42: 831, 43: 1029, 44: 239, 45: 328, 46: 239, 47: 452, 48: 300, 49: 825, 50: 831, 51: 831, 52: 831, 53: 660, 54: 27, 55: 896, 56: 1051, 57: 896, 58: 896, 59: 896, 60: 488, 61: 452, 62: 250, 63: 896, 64: 588, 65: 239, 66: 272, 67: 896, 68: 831, 69: 239, 70: 133, 71: 831, 72: 412, 73: 1010, 74: 1083, 75: 1031, 76: 409, 77: 674, 78: 516, 79: 130, 80: 521, 81: 20, 82: 897, 83: 74, 84: 17, 85: 896, 86: 452, 87: 156, 88: 896, 89: 852, 90: 896, 91: 504, 92: 25, 93: 284, 94: 831, 95: 239, 96: 831, 97: 531, 98: 453, 99: 452, 100: 896, 101: 831, 102: 939, 103: 896, 104: 588, 105: 113, 106: 831, 107: 896, 108: 896, 109: 831, 110: 152, 111: 896, 112: 306, 113: 896, 114: 896, 115: 831, 116: 1083, 117: 404, 118: 831, 119: 831, 120: 831, 121: 875, 122: 1018, 123: 896, 124: 831, 125: 351, 126: 875, 127: 831, 128: 831, 129: 660, 130: 831, 131: 831, 132: 831, 133: 896, 134: 896, 135: 582, 136: 896, 137: 323, 138: 896, 139: 896, 140: 896, 141: 896, 142: 292, 143: 831, 144: 774, 145: 235, 146: 239, 147: 831, 148: 398, 149: 896, 150: 710, 151: 239, 152: 831, 153: 896, 154: 994, 155: 152, 156: 219, 157: 907, 158: 776, 159: 116, 160: 583, 161: 958, 162: 831, 163: 25, 164: 239, 165: 896, 166: 896, 167: 896, 168: 896, 169: 896, 170: 831, 171: 831, 172: 643, 173: 896, 174: 963, 175: 896, 176: 831, 177: 363, 178: 896, 179: 831, 180: 793, 181: 831, 182: 831, 183: 239, 184: 896, 185: 519, 186: 896, 187: 896, 188: 853, 189: 2, 190: 265, 191: 917, 192: 642, 193: 831, 194: 504, 195: 433, 196: 472, 197: 896, 198: 671, 199: 831, 200: 467, 201: 896, 202: 896, 203: 30, 204: 896, 205: 353, 206: 710, 207: 452, 208: 758, 209: 831, 210: 932, 211: 166, 212: 831, 213: 896, 214: 831, 215: 968, 216: 896, 217: 831, 218: 472, 219: 831, 220: 831, 221: 709, 222: 831, 223: 831, 224: 1010, 225: 372, 226: 896, 227: 896, 228: 178, 229: 1021, 230: 21, 231: 831, 232: 896, 233: 132, 234: 570, 235: 831, 236: 736, 237: 896, 238: 831, 239: 831, 240: 831, 241: 49, 242: 1046, 243: 831, 244: 167, 245: 831, 246: 896, 247: 896, 248: 831, 249: 896, 250: 831, 251: 229, 252: 831, 253: 896, 254: 898, 255: 896, 256: 831, 257: 831, 258: 25, 259: 831, 260: 588, 261: 999, 262: 148, 263: 896, 264: 1043, 265: 494, 266: 434, 267: 143, 268: 269, 269: 831, 270: 951, 271: 831, 272: 831, 273: 896, 274: 703, 275: 1091, 276: 452, 277: 896, 278: 829, 279: 461, 280: 239, 281: 831, 282: 831, 283: 434, 284: 896, 285: 558, 286: 831, 287: 152, 288: 908, 289: 694, 290: 573, 291: 998, 292: 472, 293: 831, 294: 710, 295: 643, 296: 994, 297: 896, 298: 48, 299: 831, 300: 908, 301: 239, 302: 896, 303: 363, 304: 693, 305: 363, 306: 896, 307: 896, 308: 896, 309: 704, 310: 2, 311: 831, 312: 1081, 313: 393, 314: 831, 315: 896, 316: 896, 317: 896, 318: 515, 319: 831, 320: 831, 321: 831, 322: 896, 323: 720, 324: 831, 325: 831, 326: 896, 327: 831, 328: 896, 329: 831, 330: 896, 331: 30, 332: 972, 333: 831, 334: 230, 335: 404, 336: 831, 337: 21, 338: 831, 339: 831, 340: 831, 341: 896, 342: 896, 343: 990, 344: 896, 345: 831, 346: 338, 347: 248, 348: 831, 349: 896, 350: 1069, 351: 831, 352: 978, 353: 452, 354: 831, 355: 709, 356: 831, 357: 780, 358: 595, 359: 831, 360: 831, 361: 565, 362: 896, 363: 896, 364: 831, 365: 794, 366: 831, 367: 428, 368: 589, 369: 896, 370: 46, 371: 875, 372: 831, 373: 896, 374: 896, 375: 212, 376: 1014, 377: 831, 378: 211, 379: 831, 380: 831, 381: 239, 382: 428, 383: 831, 384: 994, 385: 1014, 386: 340, 387: 831, 388: 831, 389: 831, 390: 831, 391: 896, 392: 896, 393: 831, 394: 831, 395: 896, 396: 167, 397: 896, 398: 831, 399: 831, 400: 831, 401: 133, 402: 896, 403: 903, 404: 831, 405: 727, 406: 872, 407: 831, 408: 831, 409: 261, 410: 99, 411: 825, 412: 831, 413: 710, 414: 831, 415: 831, 416: 896, 417: 363, 418: 933, 419: 1002, 420: 831, 421: 831, 422: 156, 423: 605, 424: 896, 425: 853, 426: 213, 427: 292, 428: 831, 429: 398, 430: 679, 431: 451, 432: 363, 433: 831, 434: 831, 435: 896, 436: 896, 437: 412, 438: 471, 439: 831, 440: 219, 441: 896, 442: 992, 443: 239, 444: 896, 445: 896, 446: 897, 447: 908, 448: 970, 449: 831, 450: 712, 451: 831, 452: 831, 453: 831, 454: 302, 455: 932, 456: 831, 457: 831, 458: 452, 459: 896, 460: 831, 461: 825, 462: 831, 463: 831, 464: 831, 465: 896, 466: 896, 467: 363, 468: 896, 469: 896, 470: 831, 471: 831, 472: 896, 473: 831, 474: 272, 475: 896, 476: 588, 477: 831, 478: 896, 479: 896, 480: 340, 481: 825, 482: 896, 483: 896, 484: 831, 485: 831, 486: 831, 487: 392, 488: 986, 489: 831, 490: 896, 491: 444, 492: 393, 493: 896, 494: 1083, 495: 831, 496: 896, 497: 831, 498: 896, 499: 898, 500: 253, 501: 896, 502: 831, 503: 2, 504: 831, 505: 363, 506: 642, 507: 831, 508: 896, 509: 363, 510: 1014, 511: 896, 512: 831, 513: 110, 514: 896, 515: 831, 516: 831, 517: 896, 518: 559, 519: 568, 520: 1044, 521: 896, 522: 896, 523: 896, 524: 896, 525: 831, 526: 312, 527: 896, 528: 896, 529: 908, 530: 328, 531: 933, 532: 645, 533: 896, 534: 831, 535: 896, 536: 831, 537: 805, 538: 267, 539: 880, 540: 896, 541: 259, 542: 642, 543: 544, 544: 896, 545: 831, 546: 784, 547: 839, 548: 312, 549: 831, 550: 574, 551: 831, 552: 643, 553: 760, 554: 831, 555: 831, 556: 124, 557: 612, 558: 896, 559: 896, 560: 594, 561: 434, 562: 2, 563: 831, 564: 908, 565: 106, 566: 896, 567: 831, 568: 831, 569: 675, 570: 896, 571: 896, 572: 990, 573: 197, 574: 99, 575: 152, 576: 660, 577: 896, 578: 581, 579: 896, 580: 475, 581: 831, 582: 896, 583: 239, 584: 825, 585: 933, 586: 146, 587: 896, 588: 831, 589: 921, 590: 2, 591: 896, 592: 588, 593: 825, 594: 831, 595: 896, 596: 869, 597: 594, 598: 831, 599: 831, 600: 831, 601: 176, 602: 27, 603: 212, 604: 896, 605: 831, 606: 831, 607: 1006, 608: 831, 609: 831, 610: 831, 611: 831, 612: 896, 613: 897, 614: 300, 615: 831, 616: 963, 617: 49, 618: 896, 619: 1081, 620: 908, 621: 831, 622: 86, 623: 896, 624: 831, 625: 896, 626: 769, 627: 831, 628: 896, 629: 896, 630: 896, 631: 896, 632: 828, 633: 831, 634: 831, 635: 896, 636: 896, 637: 583, 638: 241, 639: 133, 640: 239, 641: 831, 642: 831, 643: 831, 644: 896, 645: 896, 646: 896, 647: 831, 648: 17, 649: 831, 650: 1091, 651: 682, 652: 896, 653: 831, 654: 955, 655: 190, 656: 265, 657: 967, 658: 831, 659: 831, 660: 831, 661: 489, 662: 831, 663: 896, 664: 896, 665: 951, 666: 896, 667: 831, 668: 521, 669: 525, 670: 299, 671: 1002, 672: 896, 673: 896, 674: 831, 675: 831, 676: 423, 677: 388, 678: 239, 679: 896, 680: 896, 681: 471, 682: 332, 683: 831, 684: 767, 685: 831, 686: 896, 687: 896, 688: 239, 689: 133, 690: 896, 691: 521, 692: 340, 693: 896, 694: 831, 695: 896, 696: 212, 697: 822, 698: 896, 699: 908, 700: 764, 701: 88, 702: 896, 703: 556, 704: 831, 705: 998, 706: 896, 707: 896, 708: 412, 709: 896, 710: 896, 711: 831, 712: 207, 713: 269, 714: 955, 715: 778, 716: 831, 717: 896, 718: 831, 719: 247, 720: 588, 721: 990, 722: 1080, 723: 363, 724: 896, 725: 831, 726: 146, 727: 896, 728: 896, 729: 831, 730: 831, 731: 40, 732: 521, 733: 831, 734: 753, 735: 494, 736: 831, 737: 831, 738: 1091, 739: 82, 740: 831, 741: 325, 742: 896, 743: 760, 744: 299, 745: 831, 746: 999, 747: 831, 748: 490, 749: 312, 750: 896, 751: 831, 752: 831, 753: 831, 754: 240, 755: 831, 756: 782, 757: 552, 758: 896, 759: 896, 760: 831, 761: 831, 762: 908, 763: 831, 764: 896, 765: 338, 766: 825, 767: 896, 768: 106, 769: 896, 770: 896, 771: 363, 772: 853, 773: 452, 774: 831, 775: 341, 776: 82, 777: 831, 778: 2, 779: 418, 780: 896, 781: 896, 782: 1051, 783: 231, 784: 831, 785: 643, 786: 831, 787: 831, 788: 831, 789: 660, 790: 831, 791: 831, 792: 477, 793: 831, 794: 1091, 795: 388, 796: 896, 797: 687, 798: 410, 799: 452, 800: 896, 801: 831, 802: 831, 803: 896, 804: 632, 805: 374, 806: 831, 807: 141, 808: 831, 809: 831, 810: 896, 811: 1083, 812: 754, 813: 239, 814: 363, 815: 831, 816: 709, 817: 239, 818: 831, 819: 1014, 820: 875, 821: 896, 822: 269, 823: 896, 824: 896, 825: 831, 826: 896, 827: 896, 828: 176, 829: 94, 830: 2, 831: 239, 832: 239, 833: 512, 834: 831, 835: 831, 836: 295, 837: 312, 838: 831, 839: 19, 840: 896, 841: 273, 842: 896, 843: 831, 844: 896, 845: 831, 846: 412, 847: 1050, 848: 839, 849: 896, 850: 831, 851: 831, 852: 831, 853: 831, 854: 452, 855: 810, 856: 896, 857: 383, 858: 831, 859: 896, 860: 896, 861: 710, 862: 239, 863: 588, 864: 896, 865: 812, 866: 210, 867: 656, 868: 831, 869: 896, 870: 831, 871: 831, 872: 831, 873: 239, 874: 896, 875: 831, 876: 831, 877: 641, 878: 831, 879: 831, 880: 896, 881: 831, 882: 239, 883: 896, 884: 896, 885: 1019, 886: 896, 887: 896, 888: 831, 889: 896, 890: 198, 891: 792, 892: 63, 893: 831, 894: 412, 895: 896, 896: 239, 897: 710, 898: 1004, 899: 908, 900: 896, 901: 461, 902: 831, 903: 896, 904: 778, 905: 896, 906: 831, 907: 896, 908: 831, 909: 219, 910: 388, 911: 452, 912: 934, 913: 896, 914: 2, 915: 831, 916: 62, 917: 831, 918: 831, 919: 49, 920: 595, 921: 63, 922: 831, 923: 831, 924: 179, 925: 1083, 926: 388, 927: 831, 928: 896, 929: 1089, 930: 831, 931: 954, 932: 831, 933: 896, 934: 803, 935: 341, 936: 2, 937: 831, 938: 896, 939: 831, 940: 831, 941: 710, 942: 896, 943: 363, 944: 831, 945: 1037, 946: 831, 947: 957, 948: 896, 949: 896, 950: 508, 951: 896, 952: 178, 953: 831, 954: 831, 955: 831, 956: 239, 957: 272, 958: 161, 959: 167, 960: 141, 961: 414, 962: 166, 963: 758, 964: 856, 965: 831, 966: 896, 967: 831, 968: 239, 969: 206, 970: 831, 971: 295, 972: 896, 973: 831, 974: 239, 975: 896, 976: 896, 977: 896, 978: 831, 979: 363, 980: 753, 981: 146, 982: 1043, 983: 831, 984: 794, 985: 521, 986: 896, 987: 261, 988: 1013, 989: 896, 990: 87, 991: 831, 992: 831, 993: 1028, 994: 896, 995: 185, 996: 896, 997: 232, 998: 452, 999: 261, 1000: 908, 1001: 896, 1002: 2, 1003: 896, 1004: 896, 1005: 896, 1006: 896, 1007: 896, 1008: 992, 1009: 896, 1010: 831, 1011: 660, 1012: 831, 1013: 758, 1014: 896, 1015: 896, 1016: 1083, 1017: 1014, 1018: 1031, 1019: 896, 1020: 831, 1021: 896, 1022: 831, 1023: 675, 1024: 152, 1025: 831, 1026: 39, 1027: 1083, 1028: 831, 1029: 363, 1030: 558, 1031: 761, 1032: 332, 1033: 228, 1034: 52, 1035: 831, 1036: 17, 1037: 896, 1038: 831, 1039: 831, 1040: 2, 1041: 896, 1042: 951, 1043: 897, 1044: 838, 1045: 219, 1046: 896, 1047: 645, 1048: 1010, 1049: 239, 1050: 831, 1051: 831, 1052: 896, 1053: 908, 1054: 896, 1055: 404, 1056: 896, 1057: 831, 1058: 1087, 1059: 939, 1060: 896, 1061: 831, 1062: 896, 1063: 831, 1064: 896, 1065: 831, 1066: 831, 1067: 896, 1068: 472, 1069: 39, 1070: 2, 1071: 896, 1072: 896, 1073: 760, 1074: 908, 1075: 831, 1076: 903, 1077: 896, 1078: 62, 1079: 831, 1080: 875, 1081: 831, 1082: 831, 1083: 831, 1084: 91, 1085: 720, 1086: 896, 1087: 353, 1088: 640, 1089: 1069, 1090: 831, 1091: 398, 1092: 831, 1093: 349, 1094: 831, 1095: 831, 1096: 471}\n"
     ]
    }
   ],
   "source": [
    "#1.1. For every user in the given data, find its most correlated user.\n",
    "users = {}\n",
    "for correlation in cor:\n",
    "    sort = cor[correlation].sort_values(ascending=False)\n",
    "    users[correlation] = sort.index[1]\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 831)\n",
      "0.998789092477981\n",
      "the pair of the most correlated users in the data is 896 and 831\n"
     ]
    }
   ],
   "source": [
    "#1.2\n",
    "unstack_cor = cor.unstack()\n",
    "sorts = unstack_cor.sort_values(kind=\"quicksort\")\n",
    "print(sorts.drop_duplicates().idxmax())\n",
    "print(sorts.drop_duplicates().max())\n",
    "print(\"the pair of the most correlated users in the data is 896 and 831\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998789092477981"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.3\n",
    "(cor.max(axis=1)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.337763</td>\n",
       "      <td>0.323811</td>\n",
       "      <td>0.217614</td>\n",
       "      <td>0.368250</td>\n",
       "      <td>0.422829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.563834</td>\n",
       "      <td>0.531068</td>\n",
       "      <td>0.311732</td>\n",
       "      <td>0.645176</td>\n",
       "      <td>0.664096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460725</td>\n",
       "      <td>0.785843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.661554</td>\n",
       "      <td>0.512220</td>\n",
       "      <td>0.369103</td>\n",
       "      <td>0.646632</td>\n",
       "      <td>0.729210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.224024</td>\n",
       "      <td>0.404253</td>\n",
       "      <td>0.358657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>0.255438</td>\n",
       "      <td>0.387712</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>0.484603</td>\n",
       "      <td>0.292886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.258664</td>\n",
       "      <td>0.374899</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.198358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.216660</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.340718</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.402888</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.818988</td>\n",
       "      <td>0.345736</td>\n",
       "      <td>0.385944</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.390381</td>\n",
       "      <td>0.312582</td>\n",
       "      <td>0.628361</td>\n",
       "      <td>0.648463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.108995</td>\n",
       "      <td>0.130190</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.166927</td>\n",
       "      <td>0.137077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.313826</td>\n",
       "      <td>0.574365</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.392484</td>\n",
       "      <td>0.548735</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.292073</td>\n",
       "      <td>0.529138</td>\n",
       "      <td>0.574514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.458706</td>\n",
       "      <td>0.720326</td>\n",
       "      <td>0.773501</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.365428</td>\n",
       "      <td>0.639427</td>\n",
       "      <td>0.489932</td>\n",
       "      <td>0.338069</td>\n",
       "      <td>0.598056</td>\n",
       "      <td>0.644131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>0.311671</td>\n",
       "      <td>0.521499</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.264850</td>\n",
       "      <td>0.256234</td>\n",
       "      <td>0.376801</td>\n",
       "      <td>0.177096</td>\n",
       "      <td>0.516993</td>\n",
       "      <td>0.345430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.000000  0.453172  0.460725  0.224024  0.258664  0.337763  0.323811   \n",
       "1     0.453172  0.000000  0.785843  0.404253  0.374899  0.563834  0.531068   \n",
       "2     0.460725  0.785843  0.000000  0.358657  0.443320  0.661554  0.512220   \n",
       "3     0.224024  0.404253  0.358657  0.000000  0.198358  0.255438  0.387712   \n",
       "4     0.258664  0.374899  0.443320  0.198358  0.000000  0.311742  0.216660   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1092  0.402888  0.687942  0.818988  0.345736  0.385944  0.592500  0.390381   \n",
       "1093  0.045282  0.188574  0.198594  0.101303  0.108995  0.130190  0.147645   \n",
       "1094  0.313826  0.574365  0.715653  0.366085  0.392484  0.548735  0.350640   \n",
       "1095  0.458706  0.720326  0.773501  0.279389  0.365428  0.639427  0.489932   \n",
       "1096  0.311671  0.521499  0.499990  0.261623  0.264850  0.256234  0.376801   \n",
       "\n",
       "             7         8         9  \n",
       "0     0.217614  0.368250  0.422829  \n",
       "1     0.311732  0.645176  0.664096  \n",
       "2     0.369103  0.646632  0.729210  \n",
       "3     0.267130  0.484603  0.292886  \n",
       "4     0.185430  0.340718  0.322100  \n",
       "...        ...       ...       ...  \n",
       "1092  0.312582  0.628361  0.648463  \n",
       "1093  0.106326  0.166927  0.137077  \n",
       "1094  0.292073  0.529138  0.574514  \n",
       "1095  0.338069  0.598056  0.644131  \n",
       "1096  0.177096  0.516993  0.345430  \n",
       "\n",
       "[1097 rows x 10 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4 For users 0, 1, 2, \\dots, 9, print their most correlated users.\n",
    "users_9 = cor.iloc[:, 0:10]\n",
    "users_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 314, 1: 896, 2: 831, 3: 704, 4: 660, 5: 574, 6: 521, 7: 850, 8: 412, 9: 966}\n"
     ]
    }
   ],
   "source": [
    "users = {}\n",
    "for correlation in users_9:\n",
    "    sort = users_9[correlation].sort_values(ascending=False)\n",
    "    users[correlation] = sort.index[1]\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2:\n",
    "\n",
    "We want to find a model between the ratings and the personal part of the data. To do so, consider:\n",
    "\n",
    "\n",
    "**Part 1**: the ratings of all users over columns 1-400: \n",
    "\n",
    "-- Columns 1-400: These columns contain the ratings for the 400 movies (0 to 4, and missing);\n",
    "\n",
    "call this part `df_rate`\n",
    "\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "**Part 2**:  the part of the data which includes all users over columns 401-474\n",
    "\n",
    "-- Columns 401-421: These columns contain self-assessments on sensation seeking behaviors (1-5)\n",
    "\n",
    "-- Columns 422-464: These columns contain responses to personality questions (1-5)\n",
    "\n",
    "-- Columns 465-474: These columns contain self-reported movie experience ratings (1-5)\n",
    "\n",
    "call this part `df_pers`.\n",
    "\n",
    "---\n",
    "\n",
    "Our main task is to model: \n",
    "\n",
    "\n",
    "`df_pers = function(df_rate)`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Split the original data into training and testing as the ratio 0.80: 0.20. \n",
    "\n",
    "\n",
    "2.1. Model `df_pers = function(df_rate)` by using the linear regression. \n",
    "\n",
    "What are the errors on: (i) the training part; (ii) the testing part?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.2. Model `df_pers = function(df_rate)` by using the ridge regression with hyperparamter values alpha from [0.0, 1e-8, 1e-5, 0.1, 1, 10]. \n",
    "\n",
    "For every of the previous values for alpha, what are the errors on: (i) the training part; (ii) the testing part?\n",
    "\n",
    "What is a best choice for alpha?\n",
    "\n",
    "\n",
    "\n",
    "2.3. Model `df_pers = function(df_rate)` by using the lasso regression with hyperparamter values alpha from [1e-3, 1e-2, 1e-1, 1]. \n",
    "\n",
    "For every of the previous values for alpha, what are the errors on: (i) the training part; (ii) the testing part?\n",
    "\n",
    "What is a best choice for alpha?\n",
    "\n",
    "\n",
    "**Note**: Ignore any `convergence warning` in case you may obtain in the Lasso regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('movieReplicationSet.csv')\n",
    "df_rate = data[data.columns[:400]]\n",
    "df_pers = data[data.columns[400:474]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = int(round(0.8 * len(df_rate)))\n",
    "\n",
    "df_r_trn = df_rate.iloc[:training_count]\n",
    "df_r_tst = df_rate.iloc[training_count:]\n",
    "df_p_trn = df_pers.iloc[:training_count]\n",
    "df_p_tst = df_pers.iloc[training_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_data(df):\n",
    "    ret = df.copy()\n",
    "    for i in df.columns:\n",
    "        a = df[i].mean()\n",
    "        ret.replace(\"\", value=a, inplace=True)\n",
    "        ret.replace(float(\"NaN\") ,value=a, inplace=True)\n",
    "        ret.fillna(value=a, inplace=True)\n",
    "    return ret\n",
    "\n",
    "df_r_trn = clean_raw_data(df_r_trn)\n",
    "df_r_tst = clean_raw_data(df_r_tst)\n",
    "df_p_trn = clean_raw_data(df_p_trn)\n",
    "df_p_tst = clean_raw_data(df_p_tst)\n",
    "df_p_tst = df_p_tst.reset_index()\n",
    "\n",
    "models = []\n",
    "\n",
    "for m in df_p_trn.columns:\n",
    "    model = lm.LinearRegression().fit(df_r_trn, df_p_trn[m])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set error by column\n",
      "[0.2915445259157225, 0.3897790905201783, 0.27118165877505773, 0.2734737594646397, 0.3702037032563794, 0.3383488487107422, 0.3095031926563582, 0.4159923387778303, 0.31368808748020266, 0.37958108424865256, 0.30374974741340616, 0.29895884648575116, 0.2618240588155449, 0.39055821654227324, 0.3188634269241765, 0.2236597988674901, 0.23486626298609703, 0.35415725457590236, 0.43297772929924816, 0.23939514244317833, 0.2520465448004535, 0.24188991292914114, 0.1333275639551888, 0.3168097089879289, 0.17374134929075735, 0.24728722596233413, 0.150746719756634, 0.23351899777874607, 0.2804880517777182, 0.12722439948348144, 0.21766450590840497, 0.35201165555022323, 0.1200088498419374, 0.17219299543875094, 0.15406941798027202, 0.19812061575544876, 0.22998461672754456, 0.3531393257670596, 0.21795606206125903, 0.1435317712212379, 0.2657269959024604, 0.21096518409991893, 0.2506808185782618, 0.26862592352546116, 0.2072662908102794, 0.29495342715840445, 0.29920562665644007, 0.17228263836396418, 0.19911550931231395, 0.15258962593034361, 0.20688134975737596, 0.13059464659783648, 0.16442016806574758, 0.23324041678683388, 0.21124562774583744, 0.21785185975639632, 0.31668853702433264, 0.1827168645060254, 0.21018011411190074, 0.1450725767850422, 0.3342374502088249, 0.1478492193826635, 0.23272532202631116, 0.24302496252262942, 0.2474620888302727, 0.2943609709155068, 0.42552132632764533, 0.479724305268314, 0.4264613898991663, 0.45117038349480393, 0.20649406151738, 0.22192112428377941, 0.30411303817418167, 0.441771515954564]\n",
      "\n",
      "Training set average error\n",
      "0.26385416791100774\n",
      "\n",
      "==============================\n",
      "\n",
      "Testing set error by column\n",
      "[0.9966844508312865, 0.5685248797174861, 0.6279479619323731, 1.5146136836986528, 0.6615597711329431, 0.7203843569017332, 0.6250922742427931, 0.5215884709307369, 1.1863537315364396, 0.7024579786782789, 0.7301095467184289, 0.6587827776750916, 0.5353216335069646, 1.3625219763394136, 1.1078874802320184, 0.6149107618537252, 0.49382992261038894, 0.43842142757839886, 0.7196358910708889, 0.6148697899651266, 1.2859466537150512, 0.505250076161848, 0.5834597284430728, 0.3714957896384823, 0.7821492539051502, 0.35640476662139775, 0.4100732268031, 0.3244908555321981, 0.4807207148064516, 0.6409604026202774, 0.3355811281257301, 0.4719149948505314, 1.5058938809324947, 0.2654549885938657, 0.36302232749640684, 0.32682857865654685, 0.39593449704433725, 0.5413801321873886, 0.9387411764769938, 0.38844985766887624, 0.28133080027087165, 0.5430369961394141, 0.4097079879518779, 0.5600222262620966, 0.48878959406038436, 0.44016455282194505, 0.5541684023547542, 0.6485889760991648, 0.3126885864774135, 0.43253640260106796, 0.3258427574033805, 0.31998026989460593, 0.27071398672531277, 0.2922318210971485, 0.4065474418741542, 0.4263778758804094, 0.4593678327021942, 0.7940058393792917, 0.38188843383116117, 0.39729747789335645, 0.3945117609629769, 0.8381249436875723, 0.38865392103695456, 0.5196746864790619, 0.5804674206802596, 0.44312893277000176, 0.8768685860113606, 0.7841502726649554, 1.0345390830332455, 0.806319208895582, 1.9262363464540446, 0.5493996652202185, 0.42001498277704524, 0.6009133367428089]\n",
      "\n",
      "Testing set average error\n",
      "0.6159992325212089\n"
     ]
    }
   ],
   "source": [
    "def calc_error(model, idx, x, y):\n",
    "    error = None\n",
    "    tmp = []\n",
    "    y_pred = model.predict(x)\n",
    "    for pred_idx in range(len(y_pred)):\n",
    "        tmp.append(\n",
    "            abs(\n",
    "                (\n",
    "                    y[y.columns[idx]][pred_idx] - y_pred[pred_idx]\n",
    "                ) / y[y.columns[idx]][pred_idx]\n",
    "            )\n",
    "        )\n",
    "    return np.mean(tmp)\n",
    "\n",
    "train_error = []\n",
    "for m_idx in range(len(models)):\n",
    "    model = models[m_idx]\n",
    "    err = calc_error(model, m_idx, df_r_trn, df_p_trn)\n",
    "    train_error.append(err)\n",
    "\n",
    "\n",
    "test_error = []\n",
    "for m_idx in range(len(models)):\n",
    "    model = models[m_idx]\n",
    "    err = calc_error(model, m_idx, df_r_tst, df_p_tst)\n",
    "    test_error.append(err)\n",
    "\n",
    "print(\"Training set error by column\")\n",
    "print(train_error)\n",
    "print()\n",
    "print(\"Training set average error\")\n",
    "print(np.mean(train_error))\n",
    "print()\n",
    "print(\"==============================\")\n",
    "print()\n",
    "print(\"Testing set error by column\")\n",
    "print(test_error)\n",
    "print()\n",
    "print(\"Testing set average error\")\n",
    "print(np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2\n",
    "hyperprarams = [0.0, 1e-8, 1e-5, 0.1, 1, 10]\n",
    "models_by_param = {}\n",
    "\n",
    "for param in hyperprarams:\n",
    "    ridges = []\n",
    "    for y in df_p_trn.columns:\n",
    "        ridges.append(lm.Ridge(alpha=param).fit(df_r_trn, df_p_trn[y]))\n",
    "    models_by_param[param] = ridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 0.0\n",
      "\tTraining set error by column\n",
      "[0.2915445259157234, 0.38977909052017906, 0.27118165877505646, 0.2734737594646397, 0.3702037032563805, 0.33834884871074183, 0.3095031926563575, 0.41599233877782926, 0.3136880874802027, 0.37958108424865167, 0.30374974741340605, 0.2989588464857514, 0.26182405881554527, 0.3905582165422747, 0.3188634269241775, 0.22365979886749018, 0.23486626298609636, 0.3541572545759011, 0.4329777292992487, 0.23939514244317864, 0.2520465448004541, 0.24188991292914125, 0.13332756395518872, 0.3168097089879294, 0.17374134929075732, 0.24728722596233355, 0.1507467197566338, 0.23351899777874555, 0.280488051777717, 0.1272243994834814, 0.21766450590840533, 0.3520116555502233, 0.12000884984193741, 0.17219299543875002, 0.1540694179802719, 0.19812061575544948, 0.22998461672754436, 0.3531393257670602, 0.21795606206125798, 0.14353177122123797, 0.26572699590246035, 0.21096518409991838, 0.25068081857826174, 0.2686259235254598, 0.2072662908102794, 0.2949534271584038, 0.2992056266564395, 0.17228263836396376, 0.19911550931231375, 0.15258962593034325, 0.20688134975737557, 0.1305946465978368, 0.164420168065748, 0.23324041678683397, 0.2112456277458384, 0.2178518597563965, 0.3166885370243315, 0.18271686450602517, 0.21018011411190082, 0.14507257678504218, 0.3342374502088257, 0.14784921938266374, 0.23272532202631174, 0.24302496252262945, 0.24746208883027326, 0.29436097091550656, 0.4255213263276435, 0.47972430526831256, 0.4264613898991666, 0.45117038349480465, 0.20649406151737923, 0.22192112428377925, 0.3041130381741821, 0.4417715159545641]\n",
      "\tAverage training set error by column\n",
      "0.26385416791100763\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966844508312865, 0.5685248797174749, 0.6279479619323651, 1.5146136836986386, 0.6615597711329508, 0.7203843569017391, 0.625092274242803, 0.5215884709307382, 1.1863537315364205, 0.7024579786782779, 0.7301095467184338, 0.6587827776750774, 0.5353216335069593, 1.3625219763394203, 1.1078874802320189, 0.6149107618537303, 0.49382992261039343, 0.43842142757840663, 0.7196358910708972, 0.6148697899651365, 1.2859466537150759, 0.5052500761618538, 0.5834597284430703, 0.3714957896384817, 0.7821492539051401, 0.35640476662140086, 0.4100732268030941, 0.3244908555322055, 0.48072071480645445, 0.640960402620279, 0.3355811281257176, 0.4719149948505403, 1.5058938809324964, 0.2654549885938655, 0.3630223274964047, 0.32682857865654497, 0.39593449704433725, 0.5413801321873919, 0.9387411764770094, 0.3884498576688834, 0.28133080027087815, 0.5430369961394207, 0.409707987951881, 0.5600222262621042, 0.488789594060381, 0.4401645528219498, 0.5541684023547598, 0.6485889760991601, 0.3126885864774198, 0.43253640260106563, 0.3258427574033784, 0.3199802698946058, 0.2707139867253119, 0.29223182109715296, 0.406547441874151, 0.4263778758804139, 0.4593678327021861, 0.7940058393792896, 0.381888433831167, 0.39729747789336417, 0.3945117609629752, 0.8381249436875612, 0.38865392103695073, 0.5196746864790572, 0.5804674206802631, 0.4431289327700034, 0.8768685860113793, 0.7841502726649507, 1.0345390830332655, 0.8063192088955841, 1.9262363464540577, 0.549399665220222, 0.4200149827770486, 0.6009133367428204]\n",
      "\tAverage testing set error by column\n",
      "0.6159992325212106\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 1e-08\n",
      "\tTraining set error by column\n",
      "[0.2915445259419812, 0.38977909056740545, 0.2711816587545348, 0.2734737594891145, 0.3702037033108638, 0.33834884873410503, 0.30950319268156423, 0.41599233879164454, 0.3136880875039008, 0.37958108427377424, 0.3037497474388027, 0.2989588464963255, 0.26182405881139964, 0.3905582165522116, 0.3188634269572518, 0.22365979887465232, 0.23486626301503116, 0.354157254582796, 0.4329777293267598, 0.23939514243096816, 0.25204654480968836, 0.24188991294574644, 0.13332756395494696, 0.31680970901168304, 0.17374134930291385, 0.24728722598246983, 0.15074671976036919, 0.23351899779802104, 0.28048805180594, 0.12722439948683872, 0.21766450592061032, 0.3520116555411517, 0.12000884984343001, 0.1721929954526515, 0.1540694179824471, 0.1981206157604875, 0.2299846167428708, 0.353139325790973, 0.21795606208499216, 0.14353177122729355, 0.2657269959211306, 0.21096518411288506, 0.2506808186018966, 0.2686259235492772, 0.20726629082127468, 0.29495342718057194, 0.2992056267033359, 0.17228263838000854, 0.19911550931840066, 0.1525896259445557, 0.20688134977574688, 0.13059464660150852, 0.1644201680681202, 0.2332404167936153, 0.21124562776618724, 0.2178518597708625, 0.3166885370613663, 0.18271686452378083, 0.21018011412628457, 0.14507257678829769, 0.33423745022818574, 0.1478492193883703, 0.23272532204080254, 0.24302496254098938, 0.24746208884470405, 0.2943609709275048, 0.42552132636049345, 0.4797243052886263, 0.42646138993760446, 0.451170383516162, 0.20649406153722127, 0.22192112430247565, 0.3041130381822288, 0.44177151592722863]\n",
      "\tAverage training set error by column\n",
      "0.26385416792665295\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966844508309463, 0.5685248796745741, 0.6279479616036059, 1.514613683555059, 0.66155977077666, 0.7203843563801491, 0.6250922738804714, 0.5215884705949917, 1.1863537311621744, 0.7024579785250131, 0.7301095465318438, 0.6587827772459292, 0.5353216333191158, 1.3625219755858513, 1.1078874794532907, 0.6149107615398249, 0.49382992241221896, 0.43842142732756584, 0.7196358901247141, 0.6148697897356957, 1.2859466535760788, 0.5052500759595778, 0.5834597282501928, 0.37149578960324664, 0.7821492537927877, 0.3564047664259123, 0.41007322670656426, 0.32449085533368843, 0.4807207146092153, 0.6409604025383743, 0.3355811277297127, 0.4719149944976152, 1.5058938809656424, 0.2654549881904413, 0.3630223272718825, 0.32682857841758156, 0.39593449695716776, 0.5413801318061425, 0.9387411760764376, 0.38844985749792227, 0.28133080012645206, 0.5430369958096035, 0.4097079877901218, 0.5600222258325618, 0.4887895939299625, 0.44016455248491404, 0.554168401803797, 0.6485889758842485, 0.31268858619969153, 0.4325364024559454, 0.32584275717448785, 0.31998026968547105, 0.270713986434928, 0.2922318209244133, 0.4065474415719608, 0.42637787568608104, 0.4593678324156288, 0.7940058388460394, 0.38188843360861696, 0.3972974777168843, 0.3945117607160333, 0.838124943391026, 0.38865392077685984, 0.5196746862383721, 0.5804674203209226, 0.44312893248812396, 0.8768685854281101, 0.7841502721698712, 1.0345390823899838, 0.8063192084606122, 1.9262363454597697, 0.5493996647352858, 0.42001498252964914, 0.6009133363338014]\n",
      "\tAverage testing set error by column\n",
      "0.6159992322200826\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 1e-05\n",
      "\tTraining set error by column\n",
      "[0.2915445521733871, 0.3897791377473134, 0.2711816382529438, 0.27347378393904703, 0.3702037577394766, 0.3383488720746393, 0.3095032178628326, 0.41599235259410633, 0.3136881111777517, 0.3795811093710999, 0.30374977280982635, 0.2989588570598678, 0.2618240546698897, 0.390558226480143, 0.3188634599988515, 0.2236598060297202, 0.23486629192108086, 0.3541572614685318, 0.432977756809087, 0.23939513023268955, 0.2520465540346891, 0.24188992953384922, 0.13332756371350787, 0.3168097327416169, 0.1737413614470934, 0.24728724609857738, 0.15074672349213084, 0.23351901705408099, 0.28048808000078224, 0.12722440284079128, 0.2176645181132622, 0.35201164647835986, 0.12000885133454546, 0.17219300934006224, 0.1540694201553999, 0.19812062079349263, 0.22998463205430947, 0.3531393496788805, 0.21795608579548237, 0.14353177727679806, 0.2657270145727042, 0.21096519706638522, 0.25068084221294784, 0.26862594734237066, 0.20726630180556116, 0.2949534493266409, 0.29920567355307714, 0.17228265440844556, 0.19911551539925654, 0.15258964014261947, 0.2068813681285221, 0.1305946502695263, 0.16442017043793597, 0.2332404235680464, 0.21124564809481192, 0.21785187422234062, 0.31668857405919554, 0.1827168822617245, 0.2101801284959794, 0.1450725800404198, 0.3342374695681197, 0.1478492250893109, 0.23272533651734884, 0.24302498088248073, 0.24746210326120233, 0.2943609829139156, 0.42552135917802597, 0.4797243255817536, 0.4264614283368735, 0.4511704048529651, 0.20649408135962585, 0.22192114298017146, 0.30411304622073443, 0.44177148861802934]\n",
      "\tAverage training set error by column\n",
      "0.2638541835562036\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966844504911792, 0.5685248368141336, 0.6279476331738244, 1.5146135401244083, 0.6615594148408556, 0.7203838353141098, 0.6250919119141164, 0.5215881351877636, 1.186353357283396, 0.7024578254116629, 0.7301093601274983, 0.65878234852571, 0.5353214456650651, 1.3625212227801942, 1.1078867015099492, 0.614910447950832, 0.493829724435137, 0.4384211767394141, 0.7196349448923686, 0.6148695605254519, 1.2859465147180018, 0.505249873885607, 0.5834595355677854, 0.3714957544035614, 0.7821491415522773, 0.356404571132563, 0.41007313027490805, 0.3244906570164445, 0.48072051756825873, 0.6409603207155684, 0.3355807321213952, 0.4719146419275009, 1.505893914078729, 0.2654545851691695, 0.3630221029764693, 0.3268283396936662, 0.39593440987627304, 0.5413797509418902, 0.9387407759108162, 0.3884496867076149, 0.28133065584479927, 0.5430366663226125, 0.4097078261931983, 0.560021796723693, 0.48878946364188397, 0.44016421578853426, 0.5541678513980774, 0.6485887611896782, 0.31268830874818626, 0.43253625748175073, 0.32584252851248724, 0.3199800607596072, 0.27071369634389975, 0.29223164835863796, 0.4065471396853465, 0.4263776815477305, 0.4593675461456988, 0.7940053061370682, 0.3818882112842247, 0.39729730141675723, 0.39451151402118534, 0.838124647154962, 0.3886536609477965, 0.5196744457936118, 0.5804670613406984, 0.44312865088815456, 0.8768680027424164, 0.7841497775871938, 1.0345384397531834, 0.8063187739235465, 1.926235352176208, 0.5493991802845357, 0.42001473537769013, 0.6009129277241962]\n",
      "\tAverage testing set error by column\n",
      "0.6159989313943894\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 0.1\n",
      "\tTraining set error by column\n",
      "[0.29180186172043454, 0.390248453455579, 0.2710224373128467, 0.27375307290288936, 0.3707376007632173, 0.33863813997583436, 0.3097634599971504, 0.416138766467776, 0.313923506838722, 0.3798257447684997, 0.30405724923373717, 0.29912675944601574, 0.2618053466621917, 0.3906536875464929, 0.3191917862961839, 0.22376701738719815, 0.23515949371950806, 0.3542647648180022, 0.4332661552517735, 0.239288166524755, 0.25214443638931344, 0.24207397202518105, 0.1333334110046257, 0.31703893452998694, 0.1738939700863054, 0.2475006645464928, 0.15078711170626635, 0.23370784097842212, 0.28076306613083263, 0.12726098853905798, 0.21781204504653173, 0.35201097075148796, 0.12002774627972947, 0.1723287949705568, 0.1540913129738235, 0.19817296248428637, 0.2301452200354178, 0.3533841751234822, 0.2181868780856059, 0.14359760119705567, 0.26592341952181436, 0.21109959048628646, 0.2509342420894749, 0.26888074546048996, 0.20737393703255183, 0.29516729070672354, 0.2996809330220425, 0.1724428815307577, 0.19918900525795238, 0.152734574290182, 0.20706979088969954, 0.13064341733728507, 0.16445180903997483, 0.23331492112954152, 0.21145772118812878, 0.21799631247068044, 0.3170646598563563, 0.1829004940596091, 0.21034200722163016, 0.1451103896123607, 0.3344257260535535, 0.14791997350769506, 0.23286709157316796, 0.24320606880244827, 0.24760272883107576, 0.2944812442102658, 0.425879509373604, 0.47996956245629707, 0.4268502852669277, 0.45138965271949394, 0.20670940203144875, 0.22210849720361447, 0.3041945587986848, 0.4415466797794754]\n",
      "\tAverage training set error by column\n",
      "0.26402195536195355\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966811759330071, 0.5680373025131362, 0.624811496306414, 1.513163356455253, 0.6581141349403375, 0.7154503913677928, 0.6216919816038955, 0.5185916737331286, 1.182810439803178, 0.7009302792101441, 0.728301935414804, 0.6546880529508248, 0.5335673966271887, 1.3553713899544482, 1.1005309276142412, 0.6119082738140585, 0.49184707860293986, 0.4360778259931632, 0.7107700786406494, 0.6126673157803686, 1.284500924976205, 0.5032703932283811, 0.5816445169979405, 0.3711179907547812, 0.7810657827704759, 0.35449067078691304, 0.40913920121142117, 0.32256953343319367, 0.4788136592177751, 0.6401787940703824, 0.33188220167566507, 0.4685529550805299, 1.5061569443832907, 0.26169232210528826, 0.3608802376350961, 0.32450689791276854, 0.3950703658157171, 0.5377348732011141, 0.9350257858135644, 0.38678871483376626, 0.27989315317124136, 0.5398952184791422, 0.40812642891033923, 0.5565490458451969, 0.4875224360766878, 0.43712013378827075, 0.5494880107589238, 0.6464912441354533, 0.3100789400408323, 0.4311291188691002, 0.32366389872098356, 0.31797814163482085, 0.268080530737075, 0.2905524113705756, 0.4036661209280016, 0.42448869230161435, 0.45663559764587347, 0.7889611147603771, 0.37970260711008086, 0.39565187837476934, 0.39211982242217963, 0.835423770655688, 0.3861581909994991, 0.5173331471458021, 0.5770502230959333, 0.4404335804921246, 0.8712303118714982, 0.7794811865587336, 1.028446771012216, 0.8021200596065146, 1.9170063975571237, 0.5448498016248897, 0.4178013630926192, 0.5969585966841647]\n",
      "\tAverage testing set error by column\n",
      "0.6131511245897513\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 1\n",
      "\tTraining set error by column\n",
      "[0.2941914737568166, 0.3945853204167454, 0.27109828728111696, 0.2764551215396313, 0.37493991571287955, 0.34119558065700506, 0.31245307897992997, 0.4183064688605582, 0.3169986357597188, 0.3823859945500303, 0.30702560995081996, 0.3015929460857995, 0.262606146943527, 0.39213953132718316, 0.32256308837321673, 0.22509979091969268, 0.23778751307799975, 0.35585408108452016, 0.4372753818102126, 0.2390815934495763, 0.2536217619496358, 0.24383313494146647, 0.13363082241141877, 0.31872734610111797, 0.17525499745127968, 0.2494496176293969, 0.15157926774561706, 0.23551480112419648, 0.283090016841917, 0.12787317945778237, 0.2194857669838211, 0.3540335040683742, 0.12045693233948467, 0.17394326623084247, 0.15447293914538596, 0.19927490624250427, 0.23178371314471366, 0.3555570872559315, 0.2200836184058359, 0.14437826903477793, 0.26836934642089894, 0.21275019577763538, 0.25364984512338956, 0.2712828637415849, 0.20859844975864725, 0.2972193726817125, 0.3037443704919432, 0.1739973827385542, 0.20068216898820704, 0.1539407403186478, 0.20928209890503954, 0.13135467428351702, 0.1654071448420238, 0.23416891174067828, 0.2137124527783476, 0.21941523279739808, 0.3203340897363533, 0.184492966994517, 0.21163452710627567, 0.14584384638846717, 0.33636784972200806, 0.14871290717627658, 0.23423179816036507, 0.24513742187856097, 0.24897477171208604, 0.29577420059139403, 0.4295873607569701, 0.483251930432255, 0.4306347105318364, 0.4534856249022725, 0.2089047439939257, 0.22389954781478816, 0.30549467642635336, 0.4410104375900383]\n",
      "\tAverage training set error by column\n",
      "0.26587877259926285\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.99665927099965, 0.5622496619529527, 0.6069801876617559, 1.5011883727991189, 0.6417635066935479, 0.6880014014616574, 0.6028984028567126, 0.5012755057715654, 1.1599293914683417, 0.6899141077912387, 0.7166609720402516, 0.631553515481813, 0.523746143371307, 1.3122451616051536, 1.0580079605304824, 0.5930656433380205, 0.47691593283904277, 0.42360362587976474, 0.6605276258491973, 0.5990006153223892, 1.2729003115174071, 0.4884759037147209, 0.571376475889484, 0.3669947061405491, 0.7744463006842887, 0.34242831217295344, 0.40300912396480454, 0.31025668623685143, 0.46594031816988324, 0.6352111933077591, 0.31370003935937835, 0.44917419013624393, 1.506091977412836, 0.2411527356119534, 0.3474298387153699, 0.3090718786647383, 0.38832884587265054, 0.5142894444410637, 0.9131736262188561, 0.37523098694809615, 0.27217985832522196, 0.5230347174832936, 0.39701589509837604, 0.5388645064238443, 0.47867933469931323, 0.4192286845686327, 0.5332773161453467, 0.6324900638779255, 0.2984661890274723, 0.4215602881886268, 0.31093794770965716, 0.30561596979662814, 0.2549058710894496, 0.2791766874921748, 0.3869075164371524, 0.4117631137874079, 0.4434901388924209, 0.7590332113501536, 0.36435934504144235, 0.38699455780978975, 0.3758016741037107, 0.8222392012524692, 0.3701940781099898, 0.5008461795970989, 0.5573904973366296, 0.4266275645032811, 0.8362251316900267, 0.7518644402878181, 0.9931773141866155, 0.7739071401602603, 1.8688961402500852, 0.5192439834327427, 0.4046140210740239, 0.5753468277032904]\n",
      "\tAverage testing set error by column\n",
      "0.5963408825381922\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 10\n",
      "\tTraining set error by column\n",
      "[0.31020775302866926, 0.4213546873500194, 0.28263151330757164, 0.29704742150779806, 0.4019544900072405, 0.3581893031086437, 0.3297958810856281, 0.43608072047837376, 0.33934540326904095, 0.3985452989271648, 0.3269037214297592, 0.31936993140207187, 0.27348489645807755, 0.4151644891689194, 0.3454650300297163, 0.23739188378388767, 0.2532543842207894, 0.36956785137966613, 0.45839114555486826, 0.2464718398334979, 0.2641542405697284, 0.2579644595508679, 0.13798440609527357, 0.33089798876003396, 0.18453803285905715, 0.26190408096714873, 0.15858734290870907, 0.24978053658914015, 0.29831338220181575, 0.13388918133331396, 0.22983275574099393, 0.37196894230587746, 0.1243724306606486, 0.18525886868646796, 0.15888029590929617, 0.2124981069515745, 0.24357411713865196, 0.3733623607797023, 0.23133129996413138, 0.15110774205147628, 0.2844369018600044, 0.22457183333324612, 0.2730048047533363, 0.28709419464856784, 0.21879204875383446, 0.3104973064631867, 0.3233741436217733, 0.18543876989842237, 0.21163552597619867, 0.16286178411795496, 0.22204223455977765, 0.13934593999578873, 0.17450868702466757, 0.24450006783031955, 0.2292692476232293, 0.2317662946690849, 0.3419124642779222, 0.1946132656714503, 0.22313648168680195, 0.15131056962679088, 0.3504798020718441, 0.15351836807580807, 0.24611348766460983, 0.25808108133888386, 0.26141134475353434, 0.308883838203479, 0.4537027329447928, 0.5061120684909717, 0.4553916410389528, 0.4698750128750631, 0.2215655542586271, 0.23681158232692578, 0.3195415556004172, 0.45388525645045175]\n",
      "\tAverage training set error by column\n",
      "0.2798689744573247\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9965923064265183, 0.5223004452888543, 0.5731062527477307, 1.455366882197922, 0.5920179463554589, 0.6305778407802256, 0.5716448036657792, 0.4582289097459823, 1.1241921059117788, 0.6608738014576204, 0.6858724700380138, 0.5625256422675716, 0.49872700748865995, 1.191300209676112, 0.9716904809033783, 0.5620763808995318, 0.4314496519088703, 0.3970104191346995, 0.5646647949282567, 0.5576415618242606, 1.257127275193694, 0.4443423143373844, 0.5418642936295041, 0.3445494088046577, 0.7755438836885459, 0.30652725253079893, 0.38468319294622183, 0.2787307327378961, 0.42759911159974, 0.6310946371946613, 0.28731609654866364, 0.41403227389032654, 1.5019529022325087, 0.21884283791016568, 0.31678125293692083, 0.2659757427784636, 0.3626176060103669, 0.4522571295858059, 0.8708043191098037, 0.3417390759258812, 0.24512318042392833, 0.4930885507605625, 0.36103625772271064, 0.4944121233037492, 0.4586138883009985, 0.381799546379768, 0.501680649986085, 0.6113092657039096, 0.27867610161626477, 0.3913552313550743, 0.2831719247616953, 0.27557334976936, 0.2213431555211694, 0.2535557052333664, 0.35649166871549764, 0.379411790411179, 0.4045173444295916, 0.7062540746203906, 0.3202779912563529, 0.36737597310677783, 0.337533773777318, 0.8026599844822595, 0.32534486636537424, 0.4512484009395459, 0.5167126491670094, 0.3937823167257689, 0.7440041464319814, 0.6764909683559469, 0.9159592556576194, 0.7071978549132238, 1.7992204007909862, 0.46796486486930106, 0.3737811485213683, 0.5161124182937518]\n",
      "\tAverage testing set error by column\n",
      "0.557369217242015\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "The best train model was given by an alpha of 0.0 with an error of 0.26385416791100763\n",
      "The best test model was given by an alpha of 10 with an error of 0.557369217242015\n"
     ]
    }
   ],
   "source": [
    "min_test = (0,100)\n",
    "min_train = (0,100)\n",
    "\n",
    "for param in models_by_param:\n",
    "    train_errors = []\n",
    "    models = models_by_param[param]\n",
    "    for model_idx in range(len(models)):\n",
    "        e = calc_error(models[model_idx], model_idx, df_r_trn, df_p_trn)\n",
    "        train_errors.append(e)\n",
    "    test_errors = []\n",
    "    for model_idx in range(len(models)):\n",
    "        e = calc_error(models[model_idx], model_idx, df_r_tst, df_p_tst)\n",
    "        test_errors.append(e)\n",
    "    \n",
    "    avg_train = np.mean(train_errors)\n",
    "    avg_test = np.mean(test_errors)\n",
    "    \n",
    "    if avg_train < min_train[1]:\n",
    "        min_train = (param, avg_train)\n",
    "    if avg_test < min_test[1]:\n",
    "        min_test = (param, avg_test)\n",
    "    \n",
    "    print(f\"Results for {param}\")\n",
    "    print(\"\\tTraining set error by column\")\n",
    "    print(f\"{train_errors}\")\n",
    "    print(\"\\tAverage training set error by column\")\n",
    "    print(f\"{avg_train}\")\n",
    "    print()\n",
    "    print(\"\\t====================================\")\n",
    "    print()\n",
    "    print(\"\\tTesting set error by column\")\n",
    "    print(f\"{test_errors}\")\n",
    "    print(\"\\tAverage testing set error by column\")\n",
    "    print(f\"{avg_test}\")\n",
    "    print()\n",
    "    print()\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print(f\"The best train model was given by an alpha of {min_train[0]} with an error of {min_train[1]}\")\n",
    "print(f\"The best test model was given by an alpha of {min_test[0]} with an error of {min_test[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3\n",
    "hyperprarams = [1e-3, 1e-2, 1e-1, 1]\n",
    "lasso_models_by_param = {}\n",
    "\n",
    "for param in hyperprarams:\n",
    "    lassos = []\n",
    "    for y in df_p_trn.columns:\n",
    "        lassos.append(lm.Lasso(alpha=param).fit(df_r_trn, df_p_trn[y]))\n",
    "    lasso_models_by_param[param] = lassos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 0.001\n",
      "\tTraining set error by column\n",
      "[0.29996654986858406, 0.4031221478376354, 0.27808358458309834, 0.284038090042918, 0.38235591261153423, 0.34686192729418813, 0.3208608656791052, 0.42540449233444144, 0.3239437577038043, 0.38984879470710776, 0.31553448462471456, 0.3074053001256167, 0.2703205101476218, 0.39916134311540397, 0.3292227851802615, 0.23094733039693546, 0.24541202040551485, 0.3614530199974658, 0.4428405946071433, 0.24548300003447052, 0.258949562148379, 0.2506250197960117, 0.13611475460302075, 0.3243807671953915, 0.1798290860438673, 0.2553187832250321, 0.15621563295543203, 0.24220978361533546, 0.2915185661881014, 0.13124262741706955, 0.22473728228237957, 0.3630614388509617, 0.12377626108513012, 0.17985526584149994, 0.15761813959377852, 0.20469732548477304, 0.23723287901043286, 0.3635730055632413, 0.2252101403100402, 0.14825374429758842, 0.2747204004799094, 0.21892413851843526, 0.26108660764860825, 0.27837725550087694, 0.21400578651497462, 0.30293383617262093, 0.31233611031054903, 0.17981132617615883, 0.20614845941053017, 0.1580880820400607, 0.21566227238715313, 0.1357264831902649, 0.1707939718664526, 0.23927907875794538, 0.21964672722904513, 0.2251147404837793, 0.32956818261719834, 0.18946997039873123, 0.21676543741188387, 0.14941310435233765, 0.34128821960141575, 0.15245633280261828, 0.24063527027413215, 0.2500198813792275, 0.25298111927285494, 0.302310880412431, 0.4383953779896502, 0.49069026451202175, 0.43876598191109767, 0.4603420987974363, 0.21348108977914476, 0.22899647086331715, 0.31169122338185834, 0.4446486921287441]\n",
      "\tAverage training set error by column\n",
      "0.2719089385321419\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966289499788333, 0.5508085852843723, 0.5859192809409115, 1.4981837108559093, 0.6178447719996588, 0.6677810959470116, 0.5743735798302505, 0.48249085857562685, 1.1632451359356712, 0.6725354316507155, 0.6984528510478756, 0.6090800317693837, 0.5037600653904263, 1.2607053428618589, 1.0288911519249966, 0.5680559429665972, 0.4535532337779114, 0.4152626758806509, 0.6229608369332695, 0.5723369275417005, 1.2474674171833233, 0.46001625829538434, 0.5525713767546682, 0.3511423930274569, 0.7689440398966685, 0.3219160353472609, 0.39487509845927393, 0.28967147394766796, 0.442896890203065, 0.6289956108963113, 0.2977703453607087, 0.42854205949371493, 1.4976363841166647, 0.22501499756352455, 0.32247476477140735, 0.28579070991191247, 0.36898027721510046, 0.48085960188381016, 0.8960304367923627, 0.35450234990816254, 0.2615811153998929, 0.5071406353587752, 0.37308445747715047, 0.5144569307510816, 0.4667308044316582, 0.3944015233754547, 0.5110417616118541, 0.6185685320712692, 0.28370453871125545, 0.3986038732262078, 0.2975845016294087, 0.28683561742816305, 0.23442088902311836, 0.2635597065411677, 0.36469739229745846, 0.3989169361378743, 0.4239898920846332, 0.7270810290002764, 0.33618554115581034, 0.37204757024362345, 0.3523895468911767, 0.8051063958602127, 0.34372395571371306, 0.4746975556865303, 0.5316115596588659, 0.4141484885944578, 0.7877685560933112, 0.7159308614669074, 0.942288767972836, 0.7396388939710962, 1.852761381078396, 0.4892423573608435, 0.38088098808184095, 0.5492599337402314]\n",
      "\tAverage testing set error by column\n",
      "0.5753115063817383\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 0.01\n",
      "\tTraining set error by column\n",
      "[0.37540442038048455, 0.49463532028704416, 0.34082076080848117, 0.36282768081219924, 0.4878819132658493, 0.43201214298401547, 0.40492226910500045, 0.5042540953788964, 0.41070537732108264, 0.4667725558991293, 0.3986635758985521, 0.3852696600329083, 0.3261626728074874, 0.5061218209017918, 0.41215749759588954, 0.28663364019161197, 0.31118402290779146, 0.4391845848847371, 0.5355221018410752, 0.3167036316940071, 0.3118492185275013, 0.31865683497077213, 0.1640949518827356, 0.3903261165158756, 0.2260059091511534, 0.3180259417492815, 0.19212648193985218, 0.3068320568817833, 0.3638884499327182, 0.16458170988761447, 0.277280982019051, 0.44136990546777816, 0.14911361205962534, 0.22576031702820112, 0.19061079976369524, 0.26120650881603164, 0.29220046367517516, 0.4433511603125656, 0.2760783353126795, 0.1831303193294577, 0.3473482664434865, 0.26994310346031736, 0.3283264123903548, 0.34766793856882144, 0.2618760673680447, 0.36523937927184974, 0.39361050693060096, 0.22967862258473357, 0.2580375428050092, 0.1972109654854279, 0.26851550747747144, 0.1684255074295243, 0.21477296039449464, 0.2959600456449573, 0.280535197061976, 0.2801985964497015, 0.4104784305435697, 0.24005069580470448, 0.27779149596190117, 0.18172663923409874, 0.41756371894717287, 0.1860106540722994, 0.2910944816565279, 0.31046959288666987, 0.3066020540973019, 0.3748212622022837, 0.5324929782144315, 0.5781243271299347, 0.5285676741665989, 0.5429240152478978, 0.2611617750057753, 0.27994801347949366, 0.37188773841359624, 0.5048409693577917]\n",
      "\tAverage training set error by column\n",
      "0.3351112561677082\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9965774370444636, 0.46811003996523065, 0.555256727717027, 1.4697339810512875, 0.5641491447822747, 0.5806054904313581, 0.5519360196766858, 0.42970519812543007, 1.1319042822159953, 0.6380278581507157, 0.660970005749983, 0.4934483296697235, 0.4476410365084088, 1.0646713199345605, 0.9177106584852176, 0.557116292160379, 0.3973668870238703, 0.3724862457565434, 0.48619314913683104, 0.5617755776297827, 1.2959819811817386, 0.4187231828945916, 0.5090276517863925, 0.3194726942168171, 0.7923119851170862, 0.2647999670695692, 0.35543297540309765, 0.2561385963580981, 0.40160283108865974, 0.6379291495520868, 0.2626412260649788, 0.3925148099676281, 1.503560779825881, 0.2046235209909039, 0.28863827972751566, 0.22938045231809304, 0.3383553991540619, 0.4112251017445115, 0.874037333696562, 0.3127239555943536, 0.22513344209665748, 0.43430425122304706, 0.3326753863905601, 0.4338954805447416, 0.4490433022108427, 0.3370127805396737, 0.44108068165836983, 0.6263206972974045, 0.2523985710672786, 0.3650664481193351, 0.25849684653141014, 0.2474550925774206, 0.18846030081214057, 0.22500061384035863, 0.317573220020345, 0.33783416877259315, 0.35909567681823307, 0.6869428343266439, 0.2793909289434251, 0.34356135175314334, 0.3189037030565321, 0.7919007076979165, 0.28165074143559754, 0.4032539938198996, 0.47977623870619085, 0.37479246683439316, 0.6497380920092619, 0.6058472093437578, 0.854061478860271, 0.6562896867989441, 1.805498950226812, 0.4292393258228612, 0.35186601738888273, 0.47565759307432903]\n",
      "\tAverage testing set error by column\n",
      "0.5274827815619956\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Results for 0.1\n",
      "\tTraining set error by column\n",
      "[0.48852476183896937, 0.6309369267004212, 0.3916728187831712, 0.48113619499046223, 0.5975399234527862, 0.5321568215045566, 0.5053952488283445, 0.6353506338651566, 0.5203487935033662, 0.6143071880084051, 0.48261610002741, 0.466024237459666, 0.3688679860771408, 0.6592681312645222, 0.5220331971781453, 0.34972901157725933, 0.3776530254828369, 0.5242046654565748, 0.6698302964767704, 0.3652250508450584, 0.39573027330010213, 0.3790131852563268, 0.1730978743139543, 0.4929613422093158, 0.26188178822655905, 0.3887571223067185, 0.21946365641619398, 0.3842344049250792, 0.45038896010006196, 0.1896488601804764, 0.33739648676752637, 0.49865212928836716, 0.16471336209893683, 0.2688109735404468, 0.22692410220857342, 0.32853138714962454, 0.35930740713180476, 0.5461768044615738, 0.32896838291992386, 0.19814932142223557, 0.43328957894367526, 0.3302090497496705, 0.41554869324163207, 0.4386713512598439, 0.3059461130854921, 0.43297084584362777, 0.48712627886087506, 0.2771338188964875, 0.3251085514631201, 0.22711744275618423, 0.31823909335829076, 0.18342444998687132, 0.24966399555256327, 0.3588813263564108, 0.34001249705880976, 0.35723955126273543, 0.5065617082824979, 0.29604123035868996, 0.3544921156933503, 0.2064644739316637, 0.5432921519482293, 0.2165707304971618, 0.35039861005482453, 0.39358774551700537, 0.37953173573024845, 0.48274670912375195, 0.663466941669083, 0.7251395758432846, 0.6374503756599431, 0.6768990616148068, 0.31889544888930166, 0.3450652539962863, 0.4621221144077975, 0.6400107146551327]\n",
      "\tAverage training set error by column\n",
      "0.41155335369046186\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9965861686839832, 0.4214100358835157, 0.5615359855260307, 1.4372164444808653, 0.5394759158788877, 0.5818006857741719, 0.5308518178250601, 0.4190280606530247, 1.235353127633768, 0.6087291823325305, 0.6909435891742908, 0.5014180356806937, 0.44842673809514677, 1.0243406388426217, 0.8906942541176045, 0.5457461362181033, 0.3860279695699708, 0.3667619470325824, 0.44377281999270296, 0.5660857042163725, 1.3368820785137512, 0.3853927170171919, 0.4864233377861173, 0.313820669653673, 0.8017587499211701, 0.25941913211683887, 0.3266017600691105, 0.2511101967183438, 0.38186351250357586, 0.637125410674537, 0.24481563842462187, 0.37869719942889124, 1.5198737004723244, 0.19876467638099932, 0.27857963602386204, 0.212455111144276, 0.34021592404818624, 0.4073255921817515, 0.8740903111693737, 0.30612493031628724, 0.24037615284354796, 0.4069112880693059, 0.33072782258567845, 0.3673184182427548, 0.4437109275434194, 0.32292473034874586, 0.39643895337806145, 0.6047561557639799, 0.24024020718235656, 0.3509544231650709, 0.25662248123592823, 0.22001651671545722, 0.19123781869627546, 0.23769258800702073, 0.31183652546657714, 0.33836473112780324, 0.3405509822687654, 0.6627887550560801, 0.2666930598698899, 0.32847569700769425, 0.3262896620818318, 0.804132561293996, 0.26453504287904406, 0.40096531986014883, 0.46325686680618455, 0.36337428589287957, 0.5939039917506362, 0.5988491430695753, 0.8287616947987602, 0.6321890354775181, 1.7242133588012505, 0.41523774657227597, 0.3530958445848205, 0.48739076496960637]\n",
      "\tAverage testing set error by column\n",
      "0.5169240418322939\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1\n",
      "\tTraining set error by column\n",
      "[0.49656803982345377, 0.6396765373096536, 0.3916728187831712, 0.4949046257352273, 0.5979794181345592, 0.5323404290903107, 0.5069019299162064, 0.6424363763451238, 0.5347073619637973, 0.8127609734068986, 0.4830055361799761, 0.47914951534235367, 0.3688679860771408, 0.74267445823956, 0.5238847525890793, 0.34999116636155564, 0.3776530254828369, 0.5252772501011412, 0.6741711338646522, 0.3652250508450584, 0.398564375370076, 0.3790131852563268, 0.1732171248462035, 0.5033294166587419, 0.26188178822655905, 0.39045226807172667, 0.21971445210482812, 0.3875086797266535, 0.4581645960101737, 0.1904030845761968, 0.3420283630721729, 0.49865212928836716, 0.16471336209893683, 0.26962725915504754, 0.22692410220857342, 0.3312569712102307, 0.3609722216230959, 0.5484375772326086, 0.33019914916839926, 0.19827363533352055, 0.4343268780805172, 0.33431303100401144, 0.4170791402205724, 0.44015066806872316, 0.30745029022979603, 0.4354110013941513, 0.4934911470673806, 0.2780367442438662, 0.3251085514631201, 0.22824002571627086, 0.319836437113693, 0.18229769166232987, 0.24966399555256327, 0.36385783795657795, 0.34093402070492984, 0.3667365287784952, 0.5065617082824979, 0.29643420122964964, 0.3555509428746586, 0.2064644739316637, 0.5619343991189517, 0.21909822670828064, 0.35039861005482453, 0.40577766698248874, 0.3927110654725291, 0.49240027469125613, 0.6656203409473906, 0.7446255053436129, 0.6505791404456418, 0.6795308281026108, 0.3350333462045878, 0.3588673659955692, 0.47558759768213393, 0.6623111478636222]\n",
      "\tAverage training set error by column\n",
      "0.41958922916242114\n",
      "\n",
      "\t====================================\n",
      "\n",
      "\tTesting set error by column\n",
      "[0.9966053196528529, 0.41863374422481703, 0.5615359855260307, 1.4382699233572724, 0.5383263038163306, 0.5814862132448907, 0.5305378284127034, 0.426617774572224, 1.2188317638858792, 0.5738804362784063, 0.6918230878217869, 0.49720920239255134, 0.44842673809514677, 0.953538420171543, 0.8894725130404402, 0.5460192280242075, 0.3860279695699708, 0.36711473861191746, 0.4427020954457862, 0.5660857042163725, 1.3316982680379168, 0.3853927170171919, 0.48626240764978784, 0.31809711767520776, 0.8017587499211701, 0.25910386998213153, 0.32633182700770036, 0.25246964691315177, 0.3844963608299636, 0.6355948453956008, 0.24743396832079886, 0.37869719942889124, 1.5198737004723244, 0.19844691332738112, 0.27857963602386204, 0.21273954290047345, 0.34088399761569016, 0.40694281175481095, 0.8734293403015244, 0.3059427839012785, 0.24059647890548805, 0.4039262348311107, 0.3316116434542296, 0.3657035874610473, 0.4423938817688884, 0.32325781680664023, 0.3900084602076456, 0.6043043355683462, 0.24024020718235656, 0.34863847600131553, 0.2573708116337336, 0.21653162896539893, 0.19123781869627546, 0.24022745222495953, 0.3113555929685085, 0.33922386460851905, 0.3405509822687654, 0.6625288435539985, 0.2669014835395851, 0.32847569700769425, 0.32085171503049, 0.8015683083862457, 0.26453504287904406, 0.39711880201882105, 0.44797947493962187, 0.3585122935276057, 0.5956916271101872, 0.5900916848532264, 0.8219894747180423, 0.6337766263256589, 1.6914570744857509, 0.4165208611589344, 0.35657314722111505, 0.48590017328103163]\n",
      "\tAverage testing set error by column\n",
      "0.5141212472760577\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "The best train model was given by an alpha of 0.001 with an error of 0.2719089385321419\n",
      "The best test model was given by an alpha of 1 with an error of 0.5141212472760577\n"
     ]
    }
   ],
   "source": [
    "min_test = (0,100)\n",
    "min_train = (0,100)\n",
    "\n",
    "for param in lasso_models_by_param:\n",
    "    train_errors = []\n",
    "    models = lasso_models_by_param[param]\n",
    "    for model_idx in range(len(models)):\n",
    "        e = calc_error(models[model_idx], model_idx, df_r_trn, df_p_trn)\n",
    "        train_errors.append(e)\n",
    "    test_errors = []\n",
    "    for model_idx in range(len(models)):\n",
    "        e = calc_error(models[model_idx], model_idx, df_r_tst, df_p_tst)\n",
    "        test_errors.append(e)\n",
    "    \n",
    "    avg_train = np.mean(train_errors)\n",
    "    avg_test = np.mean(test_errors)\n",
    "    \n",
    "    if avg_train < min_train[1]:\n",
    "        min_train = (param, avg_train)\n",
    "    if avg_test < min_test[1]:\n",
    "        min_test = (param, avg_test)\n",
    "    \n",
    "    print(f\"Results for {param}\")\n",
    "    print(\"\\tTraining set error by column\")\n",
    "    print(f\"{train_errors}\")\n",
    "    print(\"\\tAverage training set error by column\")\n",
    "    print(f\"{avg_train}\")\n",
    "    print()\n",
    "    print(\"\\t====================================\")\n",
    "    print()\n",
    "    print(\"\\tTesting set error by column\")\n",
    "    print(f\"{test_errors}\")\n",
    "    print(\"\\tAverage testing set error by column\")\n",
    "    print(f\"{avg_test}\")\n",
    "    print()\n",
    "    print()\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print(f\"The best train model was given by an alpha of {min_train[0]} with an error of {min_train[1]}\")\n",
    "print(f\"The best test model was given by an alpha of {min_test[0]} with an error of {min_test[1]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
